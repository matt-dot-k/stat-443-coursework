---
title: Assignment 3
author: Matthew Kielar
date: "`r Sys.Date()`"
output:
    bookdown::pdf_document2:
        highlight: kate
        number_sections: false
        toc: false
---

```{r setup, message = FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
library(forecast)
```

## Question 1

```{r, include = FALSE, echo = FALSE}
plot_theme <- theme(
    plot.title = element_text(
        size = 16, face = "bold", hjust = 0,
        margin = margin(b = 0.1, unit = "cm")),
    plot.subtitle = element_text(
        size = 12, hjust = 0),
    axis.title = element_text(
        size = 12, face = "bold"),
    axis.text = element_text(
        size = 12),
    legend.title = element_text(
        size = 12, face = "bold"),
    legend.text = element_text(
        size = 12),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(
        linewidth = 0.7, color = "#000000"),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.y = element_line(
        linewidth = 0.1, color = "gray50"),
    panel.grid.major.y = element_line(
        linewidth = 0.1, color = "gray50"),
    panel.border = element_rect(
        color = "#000000", fill = NA, linewidth = 1.0),
    panel.background = element_rect(
        fill = "#FFFFFF"),
    plot.background = element_rect(
        fill = "#FFFFFF"),
    legend.position = "top",
    legend.justification = "left"
)

theme_set(plot_theme)
```

__Part a)__

```{r, echo = TRUE, message = FALSE}
ar_simulation <- function(alpha, mu = 2, n = 500, iter = 5000) {
    # Switch between variances
    if (alpha == 0) {
        var <- 0.25 / (1 - 0.8^2)
    } else {
        var <- 0.25
    }

    # Switch between model specifications
    if (alpha == 0) {
        mod <- list()
    } else {
        mod <- list(ar = c(alpha))
    }

    # Pre-allocate space
    mu_hat <- double(iter)

    # Run the simulation
    for (i in 1:iter) {
        simul <- arima.sim(
            model = mod, n = n, sd = sqrt(var)) + mu
        mu_hat[i] <- mean(simul)
    }
    out <- list(
        xbar = mean(mu_hat),
        sd = sd(mu_hat),
        means = mu_hat)
    return(out)
}
```

```{r, echo = TRUE, message = FALSE, warning = FALSE}
simul_1 <- ar_simulation(alpha = -0.8)
simul_2 <- ar_simulation(alpha = 0.0)
simul_3 <- ar_simulation(alpha = 0.8)
```

Shown below is the summary table with the empirical mean and standard deviation 
of $\bar{x}$

| $\alpha$ | $\bar{x}$ | $\hat{\sigma}$ |
| -------- | --------- | -------------- |
|   -0.8   |  1.9999   |     0.0127     |
|    0.0   |  2.0003   |     0.0375     |
|    0.8   |  1.9996   |     0.1113     |

__Part b)__

```{r, echo = TRUE, message = FALSE}
mean_data <- tibble(
    xbar_1 = simul_1$means,
    xbar_2 = simul_2$means,
    xbar_3 = simul_3$means,
)

palette_1 <- c("-0.8" = "#990F3D", "0.0" = "#FF7FAA", "0.8" = "#0F5499")

density_chart <- ggplot(
    data = mean_data
) +
    geom_density(
        aes(x = xbar_1, color = "-0.8"),
        linewidth = 1.2
) +
    geom_density(
        aes(x = xbar_2, color = "0.0"),
        linewidth = 1.2
) +
    geom_density(
        aes(x = xbar_3, color = "0.8"),
        linewidth = 1.2
) +
    geom_segment(
        aes(x = 2, y = 0, xend = 2, yend = 33),
        linewidth = 0.8
) +
    labs(
        x = "Sample Mean",
        y = "Density",
        title = "Sampling Densities of 3 Different AR(1) Processes",
        color = "Alpha"
) +
    scale_color_manual(
        values = palette_1
)

print(density_chart)
```

Bias doesn't look like much of a concern here since simulating 5000 AR processes 
is going to result in the grand mean being marginally different from $\mathbb{E}[X]$
to the point where it doesn't really matter. On the other hand, the variance of the
distribution of $\bar{X}$ increases with $\alpha$, which isn't surprising since a
larger AR coefficient results in greater variance in the data and hence a wider 
distribution of means across multiple simulated processes.

\clearpage

## Question 2

__Part a)__

```{r, echo = TRUE, message = FALSE, fig.width = 8, fig.height = 5}
gdp <- read_csv("./data/canadaGDP.csv", col_names = TRUE) %>%
    rename(date = `...1`) %>%
    mutate(date = ymd(date))

log_gdp <- tibble(
    date = gdp$date[13:276],
    growth = diff(log(gdp$GDP), lag = 12)
)

gdp_train <- log_gdp %>%
    filter(date >= "2010-01-01" & date <= "2017-12-01") %>%
    select(growth) %>%
    ts(start = c(2010, 01), frequency = 12)

gdp_test <- log_gdp %>%
    filter(date >= "2018-01-01") %>%
    select(growth) %>%
    ts(start = c(2018, 01), frequency = 12)

gdp_chart <- autoplot(
    gdp_train,
    linewidth = 1.2,
    color = "#990F3D"
) +
    labs(
        x = "Date",
        y = "log-Growth Rate",
        title = "log-Growth Rate of Canadian GDP"
)

print(gdp_chart)
```

```{r, echo = TRUE, message = FALSE, fig.width = 8, fig.height = 5}
acf(gdp_train, lag.max = 100, main = "ACF")
pacf(gdp_train, lag.max = 100, main = "PACF")
```

The ACF plot shows the damped sine wave pattern strongly resembled by an AR 
process. While the PACF behaves pretty erratically and has multiple cutoff 
points, it's clear that the AR($p$) class of models is suitable here. $\medskip$

__Part b)__ $\medskip$

The function below fits an AR(p) model for each value of $p$ up to and including 
``max_order``, stores the AIC for each model, and then returns the order of 
the model with the lowest AIC.

```{r, echo = TRUE, message = FALSE}
model_selector <- function(data, max_order) {
    # Pre-allocate space
    aic <- double(max_order)

    # Fit AR model for each order and get AIC
    for (i in 1:max_order) {
        mod <- arima(data, order = c(i, 0, 0), method = "ML")
        aic[i] <- mod$aic
    }
    out <- list(
        best_order = which.min(aic), aic = min(aic)
    )
    return(out)
}

model_selector(gdp_train, max_order = 12)
```

A maximum order of 12 seemed reasonable since we're dealing with monthly data. 
Based on the above output, an AR(9) model is the most suitable here. $\medskip$

__Part c)__

```{r, echo = TRUE, message = FALSE, fig.width = 8, fig.height = 8}
ar_mod <- arima(gdp_train, order = c(9, 0, 0), method = "ML")
tsdiag(ar_mod)
```

\clearpage

The diagnostic plots above indicate a reasonably well fitting model. Standardized
residuals resemble a white noise process, and the ACF of the residuals is similar 
to that of a WN process. All of the p-values for the Ljung-Box test are greater 
than 0.05, which also indicates that the data are not serially correlated. $\medskip$

__Part d)__

```{r, echo = TRUE, message = FALSE}
ar_forecast <- forecast(ar_mod, h = 24)

ar_data <- ar_forecast %>%
    as_tibble() %>%
    mutate(
        date = seq(ym("2018-01"), ym("2019-12"), by = "1 month"),
        actual = as.vector(gdp_test)) %>%
    select(-`Lo 80`, -`Hi 80`) %>%
    rename(
        forecast = `Point Forecast`,
        lower = `Lo 95`,
        upper = `Hi 95`) %>%
    relocate(c(date, actual), .before = forecast)

palette_2 <- c("Actual" = "#000000", "AR(9)" = "#0F5499", "Exponential" = "#00A0DD",
               "PI (AR)" = "#990F3D", "PI (Exp)" = "#FF7FAA")

ar_chart <- ggplot(
    data = ar_data,
    aes(x = date)
) +
    geom_line(
        aes(y = actual, color = "Actual"),
        linewidth = 1.2
) +
    geom_line(
        aes(y = forecast, color = "AR(9)"),
        linewidth = 1.2
) +
    geom_line(
        aes(y = lower, color = "PI (AR)"),
        linewidth = 1.0,
        linetype = "dashed"
) +
    geom_line(
        aes(y = upper, color = "PI (AR)"),
        linewidth = 1.0,
        linetype = "dashed"
) +
    labs(
        x = "Date",
        y = "Growth Rate",
        title = "Forecasted vs. Actual Growth Rates",
        color = "Series"
) +
    scale_color_manual(
        values = palette_2
)

print(ar_chart)
```

__Part e)__

```{r, echo = TRUE, message = FALSE}
exp_mod <- HoltWinters(
    x = gdp_train,
    start.periods = 2,
)

exp_forecast <- forecast(exp_mod, h = 24)

exp_data <- exp_forecast %>%
    as_tibble() %>%
    mutate(
        date = seq(ym("2018-01"), ym("2019-12"), by = "1 month"),
        actual = as.vector(gdp_test)) %>%
    select(-`Lo 80`, -`Hi 80`) %>%
    rename(
        forecast = `Point Forecast`,
        lower = `Lo 95`,
        upper = `Hi 95`) %>%
    relocate(c(date, actual), .before = forecast)

exp_chart <- ar_chart +
    geom_line(
        data = exp_data,
        aes(x = date, y = forecast, color = "Exponential"),
        linewidth = 1.2
) +
    geom_line(
        data = exp_data,
        aes(x = date, y = lower, color = "PI (Exp)"),
        linewidth = 1.0,
        linetype = "dashed"
) +
    geom_line(
        data = exp_data,
        aes(x = date, y = upper, color = "PI (Exp)"),
        linewidth = 1.0,
        linetype = "dashed"
)

print(exp_chart)
```

\clearpage

__Part f)__

```{r, echo = TRUE, message = FALSE}
mean_sq_err <- function(obs, pred) {
    diff_sq  <- (obs - pred)^2
    mse <- mean(diff_sq)
    return(mse)
}

mean_sq_err(ar_data$actual, ar_data$forecast)
mean_sq_err(exp_data$actual, exp_data$forecast)
```

Since exponential smoothing has a significantly wider prediction interval,
the forecast is less accurate since there's a wider possible range for it to 
fall into. Furthermore, it also has a higher MSE than the AR forecast, which
also indicates that it's slightly less accurate. $\medskip$

Although I'd be inclined to suggest the AR model in this case, both methods
have relevant pros and cons that should be considered. Exponential smoothing 
can easily deal with non-stationary data by adding extra smoothing parameters,
but requires a decent amount of data to estimate. AR models on the other hand
can be estimated reliably and are much better suited for inference.

\clearpage

## Question 3

__Part a)__

```{r, echo = TRUE, message = FALSE}
co2_data <- read_csv("./data/co2_mm_gl.csv", col_names = TRUE) %>%
    select(average) %>%
    mutate(
        date = seq(ym("1979-01"), ym("2022-10"), by = "1 month")) %>%
    relocate(date, .before = average)

co2_train <- co2_data %>%
    filter(date <= "2019-12-01") %>%
    select(average) %>%
    ts(start = c(1979, 01), frequency = 12)

co2_test  <- co2_data %>%
    filter(date >= "2020-01-01") %>%
    select(average) %>%
    ts(start = c(2020, 01), frequency = 12)
```

__Part b)__ $\medskip$

__i)__

```{r, echo = TRUE, message = FALSE, fig.height = 5, fig.width = 8}
s_diff <- diff(co2_train, lag = 12)

s_diff_chart <- autoplot(
    s_diff,
    linewidth = 1.2
) +
    labs(
        x = "Date",
        y = "Average CO2 Level",
        title = "Seasonally Differenced CO2 Levels"
)

print(s_diff_chart)
acf(s_diff, lag.max = 100, main = "ACF")
```

\clearpage

The plot of the seasonally differenced series seems to be an improvement,
although there is a bit of an upward trend in the second half. The ACF also
doesn't follow any pattern associated with the ARIMA family. As such, it seems
somewhat suitable to apply another differencing step. $\medskip$

__ii & iii)__

```{r, echo = TRUE, message = FALSE, fig.height = 5, fig.width = 8}
d_diff <- diff(s_diff, lag = 1)

d_diff_chart <- autoplot(
    d_diff,
    linewidth = 1.2
) +
    labs(
        x = "Date",
        y = "Average CO2 Level",
        title = "Differenced CO2 Levels"
)

print(d_diff_chart)
```

The new differenced series looks a lot better. There's no visible trend on
display and the series looks stationary enough for an ARIMA model. Based on 
the charts above, I would choose $d = 1$, $D = 1$, and $s = 12$. $\medskip$

__iv)__ 

```{r, echo = TRUE, message = FALSE, fig.height = 5, fig.width = 8}
acf(d_diff, lag.max = 100, main = "ACF")
pacf(d_diff, lag.max = 100, main = "PACF")
```

\clearpage

The ACF tails off like it does in the AR case with a damped since wave pattern. The
PACF is a bit more unusual but it gradually tails off as the lags increase. $\medskip$

As such, suitable order values could be as follows:

$$p = 2 \quad \quad P = 2 \quad \quad q = 1 \quad \quad Q = 1$$

__v)__

```{r, echo = TRUE, message = FALSE}
auto.arima(co2_train,
           max.p = 5, max.q = 5,
           max.P = 2, max.Q = 2,
           max.d = 1, max.D = 1,
           seasonal = TRUE
)
```

The results of the stepwise search suggest that a SARIMA$(2,1,2) \times (2,1,2)_{12}$ model
is the best one. $\medskip$

__Part c)__

```{r, echo = TRUE, message = FALSE}
arima_mod <- arima(
    co2_train, order = c(2, 1, 2), seasonal = list(
        order = c(2, 1, 2), period = 12)
)
```

__Part d)__

```{r, echo = TRUE, message = FALSE, fig.height = 8, fig.width = 8}
tsdiag(arima_mod)
```

Overall, the model seems to fit the data reasonably well. Residuals statisfy
the condition of resembling a WN process, and the ACF of residuals looks like
that of a white noise series. The p-values of the Ljung-Box test are all 
insignificant, which indicates that the data are independently distributed. $\medskip$

__Part e)__

```{r, echo = TRUE, message = FALSE}
arima_forecast <- forecast(arima_mod, h = 34)

arima_data <- arima_forecast %>%
    as_tibble() %>%
    mutate(
        date = seq(ym("2020-01"), ym("2022-10"), by = "1 month"),
        actual = as.vector(co2_test)) %>%
    select(-`Lo 80`, -`Hi 80`) %>%
    rename(
        forecast = `Point Forecast`,
        lower = `Lo 95`,
        upper = `Hi 95`) %>%
    relocate(c(date, actual), .before = forecast)

palette_3 <- c("Actual" = "#000000", "SARIMA" = "#0F5499", "PI" = "gray50")

arima_chart <- ggplot(
    data = arima_data,
    aes(x = date)
) +
    geom_line(
        aes(y = actual, color = "Actual"),
        linewidth = 1.2
) +
    geom_line(
        aes(y = forecast, color = "SARIMA"),
        linewidth = 1.2
) +
    geom_line(
        aes(y = lower, color = "PI"),
        linewidth = 1.2,
        linetype = "dashed"
) +
    geom_line(
        aes(y = upper, color = "PI"),
        linewidth = 1.2,
        linetype = "dashed"
) +
    labs(
        x = "Date",
        y = "Average CO2 Level",
        title = "Forecasted vs. Actual CO2 Levels",
        color = "Series"
) +
    scale_color_manual(
        values = palette_3
)

print(arima_chart)
```

The forecasting procedure seems to work pretty well. Forecasted values follow
the actual values quite closely and with a high degree of accuracy. Prediction
intervals aren't too wide either, which means that forecast values into the 
future are less likely to be all over the place.